{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb8d87e",
   "metadata": {},
   "source": [
    "####### ðŸš€ At the heart of every solution is a model that assumes linearity: THis means that the expected value of the target can be expressed as a weighted sum of features, or conditional mean E[Y | X= x] can be expressed as a weighted sum of the features x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05df477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17c98e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2l imported successfully!\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../')  # Add parent directory\n",
    "\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "print(\"d2l imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f1ce4",
   "metadata": {},
   "source": [
    "##### ðŸš€ Linear regression is an affine transformation of input features, which is characterized by linear transformation of features via a weighted sum, combined with a translation via the added bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0966dc",
   "metadata": {},
   "source": [
    "##### \n",
    "Note that large differences between estimates \n",
    " and targets \n",
    " lead to even larger contributions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword; while it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data). To measure the quality of a model on the entire dataset of \n",
    " examples, we simply average (or equivalently, sum) the losses on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f787a",
   "metadata": {},
   "source": [
    "####\n",
    "First, we can subsume the bias \n",
    "b into the parameter w \n",
    " by appending a column to the design matrix consisting of all 1s. Then our prediction problem is to minimize ||y - Xw||**2 .As long as the design matrix has full rank (no feature is linearly dependent on the others), then there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb9af9",
   "metadata": {},
   "source": [
    "####\n",
    "Unfortunately, SGD has drawbacks, both computational and statistical. One problem arises from the fact that processors are a lot faster multiplying and adding numbers than they are at moving data from main memory to processor cache. It is up to an order of magnitude more efficient to perform a matrixâ€“vector multiplication than a corresponding number of vectorâ€“vector operations. This means that it can take a lot longer to process one sample at a time compared to a full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10765fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f4ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
